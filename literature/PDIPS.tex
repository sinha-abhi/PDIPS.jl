\documentclass[titlepage]{abhi-tufte-handout}
% \documentclass{abhi-tufte-handout}

\title{PDIPS.jl}
\date{May 04, 2020}
\author{Abhinav Sinha}
\publisher{Computational Methods in Optimization}

\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}
\newcommand{\dollar}{\mbox{\textdollar}}

\setcounter{secnumdepth}{2}

\usepackage{booktabs}
\usepackage{makeidx}
\makeindex

%% Document
\begin{document}

\maketitle

\tableofcontents

\newpage

\begin{abstract}
\noindent
PDIPS.jl (Primal-Dual Interior-Point Solver) is a Julia package that implements
the homogeneous self-dual interior point algorithm introduced
in Tulip.jl.\cite{Tulip.jl}
This report contains the algorithm's theory, implementation details,
documentation for the code base, and some basic benchmarking results.
\end{abstract}

%% Introduction
\section{Introduction}\label{sec:introduction}
\newthought{After Karmakar's paper in 1984}, the focus of interior-point methods
(IPMs) as viable options for solving linear programs shifted to a class of
algorithms known as \textit{primal-dual methods}.\cite{Wright1997}
In standard form (or equality form), a linear program is
\begin{equation}\label{eq:primal}
\begin{array}{ll}
    \displaystyle\minimize_{\vx} & \vc^T\vx \\
    \subjectto & \mA\vx = \vb \\
               & \vx \ge 0 \\
\end{array}
\end{equation}
for \(\vc, \vx \in \RR^n\), \(\vb \in \RR^m\) and \(\mA \in \RR^{m \times n}\)
(with \(m \leq n\)).
Primal-dual methods require creating a linear program's \textit{dual}, or
\begin{equation}\label{eq:dual}
\begin{array}{ll}
    \displaystyle\maximize_{\vlambda} & \vb^T \vlambda \\
    \subjectto & \mA^T \vlambda + \vs = \vc \\
               & \vs \ge 0 \\
\end{array}
\end{equation}
for \(\vs \in \RR^n\), and \(\vlambda \in \RR^m\);
after which \eqref{eq:primal} became the \textit{primal} of a linear program.
\marginnote{Note that \(\vlambda\) is free.}
At an optimal solution \((\vx^*, \vlambda^*, \vs^*)\), where \(\vx^*\) solves
\eqref{eq:primal} and \((\vlambda^*, \vs^*)\) solves \eqref{eq:dual}, it can be
shown that
\[ \vb^T \vlambda^* = \vc^T\vx^*. \]
In fact, for any feasible vectors \((\vx, \vlambda, \vs)\), the
following property holds:
\[ \vb^T \vlambda \leq \vc^T \vx.\]

\newthought{\citet{XuHungYe1996}} extensively studied the previously
proposed homogeneous and self-dual linear feasibility model as
another way of approaching primal-dual methods.
This model requires the addition of two scalars \(\tau\) and \(\kappa\) such
that the linear program is transformed into:
\begin{equation}\label{eq:self-dual}
\begin{array}{ll}
    \displaystyle\minimize_{\vx, \vlambda, \vs, \tau, \kappa} & 0 \\
    \subjectto & \mA \vx - \tau \vb = 0 \\
               & \mA^T\vlambda + \vs - \tau \vc = 0 \\
               & \vb^T \vlambda - \vc^T \vx - \kappa = 0 \\
               & \vx, \vs, \tau, \kappa \geq 0 \\
\end{array}.
\end{equation}

\newthought{PDIPS.jl aims} to solve problems of the following form:
\begin{equation}\label{eq:problem}
\begin{array}{ll}
    \displaystyle\minimize_{\vx} & \vc^T \vx \\
    \subjectto & \mA \vx = \vb \\
               & \vl \le \vx \le \vu \\
\end{array}
\end{equation}
where \(\vl, \vu \in \RR^n\) are lower and upper bounds of \(\vx\) by taking
advantage of the homogeneous self-dual model through an algorithm described in
Tulip.jl.

%% Algorithm
\section{Homogeneous Self-Dual Algorithm}\label{sec:algorithm}
\newthought{We will begin} this section by first describing the main iteration
of this algorithm, assuming that the problem is in standard from. This includes
computing a search direction, applying centrality corrections, and termination
criteria. Dealing with bounds on \(\vx\), converting a problem to
standard form, and resolving the Newton systems
will be discussed in Section \ref{sec:solving-linear-sys}.

This algorithm takes inspiration from the predictor-corrector method published
by Mehrotra in 1992.\cite{Mehrotra1992}

\subsection{HSD Step}\label{sec:hsd-step}
Let \((\vx, \vlambda, \vs, \tau, \kappa)\) be the current strictly
feasible iterate.\footnote{%
This requirement also includes a non-negativity constraint on the
iterate.%
}
Take the following primal, dual, and gap residuals
\begin{subequations}
\begin{align*}
    r_p & = \tau \vb - \mA \vx, \\
    r_d & = \tau \vc - \mA^T \vlambda - \vs, \\
    r_g & = \vc^T \vx - \vb^T \vlambda + \kappa,
\end{align*}
\end{subequations}
and the duality measure
\marginnote{Note that this differs from the common duality measure, which is
\[ \mu = \frac{\vx^T \vs}{n}. \]}
\[ \mu = \frac{\vx^T \vs + \tau\kappa}{n + 1}. \]

To obtain a search direction
\((p_x, p_{\lambda}, p_s, p_{\tau}, p_{\kappa})\),
solve the system
\marginnote{Denote \(\mX = \diag{(\vx)}\) and \(\mS = \diag{(\vs)}\).}
\begin{subequations}
\begin{align*}
    \mA p_x - \vb p_{\tau} & = \eta r_p \\
    \mA^T p_{\lambda} + p_s - \vc p_{\tau} & = \eta r_d \\
    -\vc^T p_x + \vb^T p_{\lambda} - p_{\kappa} & = \eta r_g \\
    \mS p_x + \mX p_s & = \gamma\mu\ve - \mX\mS\ve\\
    \kappa p_{\tau} + \tau p_{\kappa} & = \gamma \mu -\tau \kappa
\end{align*}
\end{subequations}
obtained from Newton's method, where \(\gamma > 0\) is a centering parameter
and \(\eta > 0\) is a scalar.

Choose a step size \(\alpha > 0\) so that the next iterate is
\[ (\vx, \vlambda, \vs, \tau, \kappa) + \alpha
(p_x, p_{\lambda}, p_s, p_{\tau}, p_{\kappa}). \]
We also have that the residuals can be updated as
\marginnote{Although this is the case, after applying centrality correctors
to the search direction, no performance difference was found by recomputing the
residuals at the start of the next iteration.}
\begin{equation}\label{eq:residual}
(r_p^+, r_g^+, r_d^+) = (1 - \alpha \eta)(r_p, r_g, r_d).
\end{equation}

\newthought{In general,} we have the following
\[ \mu^0 = \frac{(\vx^0)^T\vz^0 + \tau^0\kappa^0}{n + 1}\text{, }
    \mu^{k+1} = \left(1 - \alpha^k\eta\right)
                [1 - \alpha^k\left(1 - \gamma -\eta\right)] \mu^k, \]
and
\[ \theta^0 = 1\text{, } \theta^{k+1} = \left(1 - \alpha^k\eta\right)\theta^k.\]
Applying this to \eqref{eq:residual}, it follows that
\[ r_p^k = \theta_k r_p^0, \]
\[ r_d^k = \theta_k r_d^0, \]
\[ r_g^k = \theta_k r_g^0. \]
This property is crucial in regards to free variables
(see Section \ref{sec:bounded-vars}).

\subsection{Search Direction Computation}\label{sec:search-dir}
The predictor-corrector method essentially computes two search directions, first
in the affine-scaling direction and then in a corrected direction to improve
the iterate's centrality.
The predictor step, along the affine-scaling direction, is computed by solving
the previous linear system with \(\eta = 1\) and the centering parameter,
\(\gamma = 0\); i.e.
% \begin{subequations}
% \begin{align*}
%     \mA p_x^{\text{aff}} - \vb p_{\tau}^{\text{aff}} & = r_p \\
%     \mA^T p_{\lambda}^{\text{aff}} + p_s^{\text{aff}} - \vc
%         p_{\tau}^{\text{aff}} & = r_d \\
%     -\vc^T p_x^{\text{aff}} + \vb^T p_{\lambda}^{\text{aff}} -
%         p_{\kappa}^{\text{aff}} & = r_g \\
%     \mS p_x^{\text{aff}} + \mX p_s^{\text{aff}} & = - \mX\mS\ve\\
%     \kappa p_{\tau}^{\text{aff}} + \tau p_{\kappa}^{\text{aff}} & = -\tau\kappa.
% \end{align*}
% \end{subequations}
\begin{equation}
\bmat{\mA & & & -\vb & & \\
      & \mA^T & \mI & -\vc & \\
      -\vc^T & \vb^T & & & -1 \\
      \mS & & \mX & & \\
      & & & \kappa & \tau }
\bmat{p_x^{\text{aff}} \\
      p_{\lambda}^{\text{aff}} \\
      p_s^{\text{aff}} \\
      p_{\tau}^{\text{aff}} \\
      p_{\kappa}^{\text{aff}}}
=
\bmat{r_p \\
      r_d \\
      r_g \\
      -\mX\mS\ve \\
      -\tau\kappa}.
\end{equation}

Ideally, we would like to choose the max step length
of \(\alpha = 1\) along the affine-scaling direction, but the non-negativity
constraint of the iterate requires us to be more judicious in our choice.
Hence, we choose
\[\alpha^{\text{aff}}
    = \max{ \left\{ 0 \leq \alpha \leq 1 \mid 0 \leq
    \left(\vx, \vs, \tau, \kappa \right) +
    \alpha \left(p_x^{\text{aff}}, p_s^{\text{aff}}, p_{\tau}^{\text{aff}},
    p_{\kappa}^{\text{aff}} \right) \right\} }.\]
Based on \(\alpha^{\text{aff}}\), let \(\eta = 1 - \gamma\), and for some
\(\beta > 0\), compute
\[ \gamma = \left(1 - \alpha^{\text{aff}}\right)^2
\min{\left(\beta, 1 - \alpha^\text{aff}\right)}.\]
When choosing \(\gamma\) and \(\eta\), we strive to ensure that
\(\gamma \leq 0.1\) (and consequently, \(\eta \geq 0.9\)) for long steps in a
search direction so that there is significant reduction in \(r_p\) and \(r_d\).

These values of \(\eta\) and \(\gamma\) dictate the corrected search direction.
Denote \(\Delta_x^{\text{aff}} = \diag{\left(\delta_x^{\text{aff}}\right)}\) and
\(\Delta_s^{\text{aff}} = \diag{\left(\delta_s^{\text{aff}}\right) }\).
The corrected search direction is computed by solving the system
% \begin{subequations}
% \begin{align*}
%     \mA p_x - \vb p_{\tau} & = \eta r_p \\
%     \mA^T p_{\lambda} + p_s - \vc p_{\tau} & = \eta r_d \\
%     -\vc^T p_x + \vb^T p_{\lambda} - p_{\kappa} & = \eta r_g \\
%     \mS p_x + \mX p_s & = \gamma\mu\ve - \mX\mS\ve
%     - \Delta_x^{\text{aff}}\Delta_s^{\text{aff}}\ve  \\
%     \kappa p_{\tau} + \tau p_{\kappa} & = \gamma\mu - \tau\kappa
%     - \delta_{\tau}^{\text{aff}} \delta_{\kappa}^{\text{aff}}.
% \end{align*}
% \end{subequations}
\begin{equation}
\bmat{\mA & & & -\vb & & \\
      & \mA^T & \mI & -\vc & \\
      -\vc^T & \vb^T & & & -1 \\
      \mS & & \mX & & \\
      & & & \kappa & \tau }
\bmat{p_x \\ p_{\lambda} \\ p_s \\ p_{\tau} \\ p_{\kappa}}
=
\bmat{\eta r_p \\
      \eta r_d \\
      \eta r_g \\
      \gamma\mu\ve - \mX\mS\ve -\Delta_x^{\text{aff}}\Delta_s^{\text{aff}}\ve \\
      \gamma\mu-\tau\kappa-\delta_{\tau}^{\text{aff}}
        \delta_{\kappa}^{\text{aff}}}.
\end{equation}

Further higher-order corrections can be computed, as suggested in Tulip.jl to
improve the centrality of the iterate. However, for the sake of maintaing a
level of simplicity in this implementation those corrections were forgone.
Including those corrections would, in general, reduce the number of iterations
the HSD algorithm requires to reach an optimal solution
(or prove infeasibility), making IPMs more competitive with the Simplex method.

\subsection{Step Size}\label{sec:step-size}
Let \((\vx, \vlambda, \vs, \tau, \kappa)\) be the current
iterate and let \({(p_x, p_{\lambda}, p_s, p_{\tau}, p_{\kappa})}\) be
the corrected search direction.
Following \citet{XuHungYe1996}, the final step size is chosen by computing
\[ \alpha_x = \min{\left\{ -\frac{\vx}{p_x} \mathrel{\Big|} p_x < 0\right\}}, \]
\[ \alpha_s = \min{\left\{ -\frac{\vs}{p_s} \mathrel{\Big|} p_s < 0\right\}}, \]
\[ \alpha_{\tau} = -\frac{\tau}{p_{\tau}}\text{, if } p_{\tau} < 0, \]
\[ \alpha_{\kappa} = -\frac{\kappa}{p_{\kappa}}\text{, if } p_{\kappa} < 0, \]
and then letting
\[ \alpha = \omega \times \min{\left\{ \alpha_x, \alpha_s, \alpha_{\tau},
   \alpha_{\kappa}\right\}}, \]
where \(0 < \omega \leq 1\) is a damping factor.

Some research has shown that IPMs can benefit from taking different step sizes
in the primal and dual case, but for the sake of simplicity, this
implementation uses the same step size.

\subsection{Starting Point}\label{sec:starting}
A universal starting point of
\[ (\vx_0, \vlambda_0, \vs_0, \tau_0, \kappa_0) = (\ve, 0, \ve, 1, 1)\]
where \(\ve\) is the vector of all ones, was suggested in
\citet{XuHungYe1996} because of simplicity and desirable properites.

IPMs tend to have issues because their central path requirements prevents them
from a warm start. This has led to them taking longer to converge to an optimal
solution. Though this algorithm addresses has certain aspects that help it
recouperate from many of those issues, it could benefit from some pre-solver
that chooses a more specialized starting point for the problem.

\subsection{Termination Criteria}\label{sec:termination}
Besides numerical instability and iteration limit, the algorithm terminates
when the iterate is proven to be optimal, or either the primal or dual is
declared infeasible.
Denote \(\varepsilon_p\), \(\varepsilon_g\), \(\varepsilon_g\),
and \(\varepsilon_i\) to be positive (specifiable) tolerances.
Choosing these positive parameters will be discussed in Section
\ref{sec:default-vals}.
Infeasibility is found when either
\[ \mu < \varepsilon_i \text{, or} \]
\[ \frac{\tau}{\kappa} < \varepsilon_i. \]
If \(\vb^T\vlambda > \varepsilon_i\), then the primal is infeasible;
if \(- \vc^T\vx > \varepsilon_i\), then the dual is infeasible.

Optimality is found when all the following conditions hold:
\[ \frac{\normof{r_p}}{\tau (1 + \normof{\vb})} < \varepsilon_p, \]
\[ \frac{\normof{r_d}}{\tau (1 + \normof{\vc})} < \varepsilon_d, \]
\[ \frac{\normof{\vc^T\vx - \vb^T\vlambda}}
        {\tau + \normof{\vb^T\vlambda})} < \varepsilon_g, \]
where \(\normof{\cdot}\) denotes \(\onormof{\cdot}{\infty}\).

\subsection{Solving Linear Systems}\label{sec:solving-linear-sys}
The crux of interior-point methods is resolving the various Newton systems at
each iteration of the algorithm. This simple implementation, for instance,
requires solving two such systems for first finding the affine-scaling
direction and then the corrected search direction. If the aforementioned
higer-order corrections were to be added, the algorithm would require an
additional system for each iteration. As a result, the performance of the
algorithm is heavily reliant on the efficiency of its linear algebra routines.

As we will soon see, the HSD algorithm gives rise to certain block-matrix
structures to which we can tailor a specialized linear system solver. Another
aspect to note is that in each system, the factorization of the left-hand side
can be reused since only the right-hand side is modified. Using a general
purpose solver makes it more difficult to efficiently resuse the Cholesky
factorizations used to solve these systems.
\marginnote{We are actually using a variant of the Cholesky factorization --
LDLT -- since some matrices can be decomposed into \(\mL\mD\mL^T\) form,
even if they are not a candidate for Cholesky.}
So, in this implementation, we have
adapted the specialized solver written for this
algorithm in Tulip.jl.\footnote{This solver can be found at
\url{https://github.com/ds4dm/Tulip.jl/tree/master/src/LinearAlgebra/LinearSolvers}.}

The Newton systems we have seen so far are of the form
\begin{equation}
\bmat{\mA & & & -\vb & & \\
      & \mA^T & \mI & -\vc & \\
      -\vc^T & \vb^T & & & -1 \\
      \mS & & \mX & & \\
      & & & \kappa & \tau }
\bmat{p_x \\ p_{\lambda} \\ p_s \\ p_{\tau} \\ p_{\kappa}}
=
\bmat{\xi_p \\
      \xi_d \\
      \xi_g \\
      \xi_{xs} \\
      \xi_{\tau\kappa}}.
\end{equation}
for given vectors
\(\xi_p, \xi_d, \xi_g, \xi_{xs}, \xi_{\tau\kappa}\).
We can compute \(\delta_s\) and \(\delta_{\kappa}\) as
\[\delta_s = \mX^{-1}\left(\xi_{xs} - \mS\delta_x\right)\text{, and} \]
\[ \delta_{\kappa} = \frac{1}{\tau}
    \left(\xi_{\tau\kappa} - \kappa\delta_{\tau}\right) \]
to get a simplified system of
\begin{equation}
\bmat{-\mM^{-1} & \mA^T & -\vc \\
      \mA & & -\vb \\
      -\vc^T & \vb^T & \tau^{-1}\kappa}
\bmat{\delta_x \\ \delta_{\lambda} \\ \delta_{\tau}}
=
\bmat{\xi_d - \mX^{-1}\xi_{xs} \\
      \xi_p \\
      \xi + \tau^{-1}\xi_{\tau\kappa}}
\end{equation}
for \(\mM = \mX\mS^{-1}\).
This system can be broken down into two augmented systems
\begin{equation}
\bmat{-\mM^{-1} & \mA^T \\ \mA & } \bmat{\vp \\ \vq} = \bmat{\vc \\ \vb}
\end{equation}
and
\begin{equation}\label{eq:augsys2}
\bmat{-\mM^{-1} & \mA^T \\ \mA & } \bmat{\vy \\ \vz}
= \bmat{\xi_d - \mX^{-1}\xi_{xs} \\ \xi_p}.
\end{equation}
We transform these two augmented systems into normal equations systems.
Equation \eqref{eq:augsys2} becomes
\begin{equation}\label{eq:normal}
\left(\mA\mM\mA^T\right) \vz = \xi_p + \xi_d - \mX^{-1}\xi_{xs}
\end{equation}
so that
\[ \vy = \mM^{-1}\left(\mA^T\vz - \xi_d + \mX^{-1}\xi_{xs} \right). \]
From this, we can recover
\[ \delta_{\tau}
    = \frac{\xi_g + \tau^{-1}\xi_{\tau\kappa} + \vc^T\vy + \vb^T\vy}
           {\tau^{-1}\kappa - \vc^T\vp + \vb^T\vq}, \]
\[ \delta_x = \vy + \delta_{\tau}\vp, \]
\[ \delta_{\lambda} = \vz + \delta_{\tau}\vq. \]

\newthought{In solving the normal equations} of the form
\[ \mA\mM\mA^T \vs = \vr, \]
we perform the an LDLT facorization of \(\mA\mM\mA^T \succ 0\), where \(\mL\)
is as lower triangular matrix with unitary elements on the diagonal and \(\mD\)
is a positive diagonal matrix. When pivoting, we check if the pivot element is
less than a certain numerical tolerance so that the current row can be dismissed
as almost linearly dependent. This introduces more numerical stability into the
algorithm.\cite{XuHungYe1996}

%% Implementation
\section{Implementation}\label{sec:implementation}
\newthought{This section contains} the implementation details for PDIPS.jl. The
focus of this implementation was on speed, even though we might have to
sacrifice robustness as a result.
Benchmark results are shown in Section \ref{sec:benchmarks}

\subsection{Bounded Variables}\label{sec:bounded-vars}
In dealing with bounded variables, we consider lower bounds, upper bounds, and
free variables. Suppose \(\vx = \left(x_1, x_2, \ldots, x_n\right)^T\).

Free variables may be split into two nonnegative components, i.e.
\[ x_i = x_i^+ - x_i^-.\]
Splitting free variables into two components has been shown to produce numerical
instability because large pairs can make \(\mA\mM\mA^T\) ill-conditioned.
However the addition of \(\eta = 1 - \gamma\) and the property from
Section \ref{sec:hsd-step} prevents the free variables from rapidly
increasing by essentially normalizing the iterates.

Lower bounds like \(l_i \leq x_i\) can be dealt with with a simple shift so that
\[ 0 \leq x_i - l_i. \]

Upper bounds, however, require more consideration. For this, we
follow the procedure recommended by Tulip.jl.\cite{Tulip.jl}
Let \(\mathcal{I} = [1, n]\) and let \(\mathcal{J} \subseteq \mathcal{I}\) be
the set of indices whose corresponding component in \(\vx\) is upper bounded.
The upper bounds are then
\[ x_j \leq u_j, \quad \forall j \in \mathcal{J}. \]
If we introduce \(\mU \in \RR^{|\mathcal{J}| \times n}\), then the upper bounds
can be expressed as \(\mU \vx \leq \vu\), where
\[
\mU_{i, j} =
\begin{cases}
1, \quad \text{if } i = j \in \mathcal{J} \\
0, \quad \text{otherwise}
\end{cases}.
\]
We apply this modification to the Newton system as
\begin{equation}
\bmat{\mA & & & & & -\vb & & \\
      \mU & \mI & & & -\vu & & \\
      & & \mA^T & \mI & -\mU^T & -\vc & \\
      -\vc^T & \vb^T & & -\vu^T & & & -1 \\
      \mS & & & \mX & & & \\
      & \mW & & & \mV & & \\
      & & & & & \kappa & \tau }
\bmat{p_x \\ p_v \\ p_{\lambda} \\ p_s \\ p_w \\ p_{\tau} \\ p_{\kappa}}
=
\bmat{\xi_p \\
      \xi_u \\
      \xi_d \\
      \xi_g \\
      \xi_{xs} \\
      \xi_{vw} \\
      \xi_{\tau\kappa}}.
\end{equation}
The method for resolving the Newton systems from Section
\ref{sec:solving-linear-sys} still holds with slight modification.

\subsection{Standard Form}\label{sec:standard-form}
Converting a linear program from the form in \eqref{eq:problem}
to standard from in \eqref{eq:primal}
requires careful implementation to ensure that constraints are not lost in
the process.

The following code block shows how PDIPS.jl counts the number of free and upper
bounded variables in \(\vx\).
\begin{dispjulia}
free = 0
ub = 0
nzv = 0 # non-zero values
for (i, (l, h)) in enumerate(zip(lp.lo, lp.hi))
    if l == -Inf && h == Inf
        # free
        free += 1
        nzv += length(lp.cols[i].ind)
    elseif isfinite(l) && isfinite(h)
        # l <= x <= h
        ub += 1
    elseif l == -Inf && isfinite(h)
        # x <= h, will be dealt with later
    elseif isfinite(l) && h == Inf
        # l <= x, will be dealt with later
    else
        error("unexpected bounds ($\dollar$l, $\dollar$h)")
    end
    nzv += length(lp.cols[i].ind)
end
\end{dispjulia}
Note that \texttt{free} is a counter for the number of free variables,
\texttt{ub} is the same for upper bounded variables, and \texttt{nzv} counts
the number of non-zero values in the columns of \(\mA\).

The following block is the method by which we modify the original problem to
account for bounded variables. Although this way of reforumlating requires
iterating over the bounds twice, it avoids the difficulty in memory management
when using a single iteration. Pre-allocating the needed memory actually gives
better performance. See Section \ref{sec:perf-notes} for more details.
\begin{dispjulia}
I = Vector{Int}(undef, nzv) # row indices
J = Vector{Int}(undef, nzv) # column indices
V = Vector{T}(undef, nzv)
ind_ub = Vector{Int}(undef, ub)
val_ub = Vector{T}(undef, ub)
b = copy(lp.b)
c = Vector{T}(undef, nv + free)

free = 0
ub = 0
nzv = 0
for (j, (l, h)) in enumerate(zip(lp.lo, lp.hi))
    column = lp.cols[j] # current column
    if l == -Inf && h == Inf
        # free variable
        c[j + free]  = lp.c[j]
        for (i,v) in zip(column.ind,column.nzval)
            nzv += 1
            I[nzv] = i
            J[nzv] = j + free
            V[nzv] = v
        end
        c[j + free + 1] = -lp.c[j]
        for (i,v) in zip(column.ind,column.nzval)
            nzv += 1
            I[nzv] = i
            J[nzv] = j + free + 1
            V[nzv] = -v
        end
        free += 1
    elseif isfinite(l) && isfinite(h)
        # l <= x <= h
        c[j + free] = lp.c[j]
        for (i,v) in zip(column.ind,column.nzval)
            b[i] -= (v * l)
            nzv += 1
            I[nzv] = i
            J[nzv] = j + free
            V[nzv] = v
        end
        ub += 1
        ind_ub[ub] = j + free
        val_ub[ub] = h - l
    elseif l == -Inf && isfinite(h)
        # x <= h
        c[j + free] = -lp.c[j]
        for (i,v) in zip(column.ind,column.nzval)
            b[i] -= (-v * u)
            nzv += 1
            I[nzv] = i
            J[nzv] = j + free
            V[nzv] = -v
        end
    elseif isfinite(l) && h == Inf
        # l <= x
        c[j + free] = lp.c[j]
        for (i,v) in zip(column.ind,column.nzval)
            b[i] -= (v * l)
            nzv += 1
            I[nzv] = i
            J[nzv] = j + free
            V[nzv] = v
        end
    else
        # this error will have been caught
    end
end
\end{dispjulia}

\subsection{Default Values}\label{sec:default-vals}
The default iteration limit is 100, but can be configured.

The default value of the damping factor in
Section \ref{sec:step-size} is
\[ \omega = 0.9995. \]
This value is not currently configurable because this value was found to be the
most practical after numerical experiments.\cite{XuHungYe1996}

The default value when computing centrality corrections with \(\gamma\) from
Section \ref{sec:search-dir} is
\[ \beta = 10^{-1}. \]
This value is also not currently configurable.

The default tolerances in Section \ref{sec:termination} are
\[ \varepsilon_p = 10^{-8}, \]
\[ \varepsilon_d = 10^{-8}, \]
\[ \varepsilon_g = 10^{-8}, \]
\[ \varepsilon_i = 10^{-8}. \]
In the implementation, this is value is coded as
\begin{dispjulia}
tol::T where T <: Real = sqrt(eps(T)))
\end{dispjulia}
since for a 64-bit floating point number,
\(\sqrt{\epsilon} \approx 1.5 \times 10^{-8}.\)
The framework for allowing each parameter to be
individually tuned is already in place, but currently, the one specified
tolerance is used for all four constants.

\section{Package Documentation}\label{sec:documentation}
\newthought{This section documents} the \texttt{PDIPS.jl} package. The package
is available at \url{https://github.com/sinha-abhi/PDIPS.jl}; it can be added
to Julia via \texttt{REPL} as
\begin{dispjulia}
] add https://github.com/sinha-abhi/PDIPS.jl
\end{dispjulia}

\subsection{API}\label{sec:api}
The API for PDIPS.jl consists of three function calls.

The following empty constructor allocates memory for an empty \texttt{Problem}
of the from described in Section \ref{sec:introduction}.
The type \texttt{T} denotes the type of the elements of the solution vector,
constraints, etc.\marginnote{For further details on types in this package, see
Section \ref{sec:types}.}
\begin{dispjulia}
Problem{T}() where T <: Real
\end{dispjulia}

\texttt{load\_problem!} loads the problem data into \texttt{lp} with no return
value. Currently, we assume that \(\mA\) is sparse when solving the
resulting linear systems.
\begin{dispjulia}
function load_problem!(
    lp::AbstractProblem{T},
    A::AbstractMatrix{T},
    b::Vector{T},
    c::Vector{T},
    lo::Vector{T},
    hi::Vector{T}
) where T <: Real
\end{dispjulia}

\texttt{solve} is the workhorse of the package; it attempts to solve \texttt{lp}
and returns a \texttt{Solution}.
\begin{dispjulia}
function solve(
    lp::AbstractProblem{T},
    maxiter::Int = 100,
    tol::T = sqrt(eps(T))
) where T <: Real
\end{dispjulia}

\subsection{Types}\label{sec:types}
\marginnote{Constructors have been omitted for brevity.}
PDIPS.jl has eight types that are used throughout the solver, and two enums to
track the solver's status.

The \texttt{Problem} type stores the number of constraints and variables the
problem has, \(\mA\) as an array of \texttt{Column}s, the right-hand side,
the cost function, and potential bounds.
\begin{dispjulia}
mutable struct Column{T}
    ind::Vector{Int}
    nzval::Vector{T}
end
\end{dispjulia}

\begin{dispjulia}
mutable struct Problem{T <: Real}
    nc::Int
    nv::Int

    cols::Vector{Column{T}} # columns of A
    b::Vector{T}
    c::Vector{T}

    lo::Vector{T}
    hi::Vector{T}
end
\end{dispjulia}
Storing the constraint matrix \(\mA\) as an array of columns makes accessing
and tracking elements easier when converting to standard from. After the
linear program is in standard from, \(\mA\) is of type \texttt{AbstractMatrix}
from SparseArrays.jl.

The \texttt{StandardProblem} type is the result of reformulating
\texttt{Problem}. Instead of explicitly storing \(\mU\), we store only the
indices with valid upper bounds and their respective values. This saves
computation time as well as memory in the algorithm.
\begin{dispjulia}
mutable struct StandardProblem{T}
    nc::Int # number of constraints
    nv::Int # number of variables
    nu::Int # number of upper-bounded variables

    upb_ind::Vector{Int} # indices of upper bounds
    upb_val::Vector{T}   # values of upper bounds

    A::SparseMatrixCSC{T}
    b::Vector{T}
    c::Vector{T}
end
\end{dispjulia}

The \texttt{Solution} type contains the solution vector \(\vx\) and
flag indicating successful optimization. It also includes information about
the problem when converted to standard form.
\begin{dispjulia}
mutable struct Solution{T}
    x::Vector{T}
    status::Bool

    # standard form
    A_std::Union{Nothing, AbstractMatrix{T}}
    b_std::Vector{T}
    c_std::Vector{T}
    x_std::Vector{T}
    $\lambda$_std::Vector{T}
    s_std::Vector{T}
end
\end{dispjulia}
However, the matrix \texttt{A\_std} will not contain all information about the
upper bounds because of the method we
used to reforumlate the problem.\footnote{See Section \ref{sec:standard-form}.}

The most important type is \texttt{Solver}, which houses information about the
status of the optimization problem, tolerances, current iterate and its
residuals, as well as a reference to the linear system solver.
\begin{dispjulia}
    mutable struct Solver{T}
    lp::StandardProblem{T}

    iter::Iterate{T}
    res::Residuals{T}
    tols::Tolerances{T}

    niter::Int # number of iterations

    # status
    status::SolverStatus
    status_primal::IterateStatus
    status_dual::IterateStatus

    # linear solver -- adapted from Tulip.jl
    ls::AbstractLinearSolver{T}
    regP::Vector{T} # primal regularization
    regD::Vector{T} # dual regularization
    regG::T         # gap regularization
end
\end{dispjulia}

The \texttt{Iterate} type has the values needed for each step from
Section \ref{sec:hsd-step}, as well as dimensions of the problem.
\begin{dispjulia}
mutable struct Iterate{T}
  nc::Int # number of constraints
  nv::Int # number of variables
  nu::Int # number of upper-bounded variables

  # primal
  x::Vector{T} # given variables
  v::Vector{T} # slacks

  # dual
  $\lambda$::Vector{T} # dual variables
  s::Vector{T} # dual slacks
  w::Vector{T} # dual upperbound slacks

  $\tau$::T # primal homogenous constant
  $\kappa$::T # dual homogenous constant

  $\mu$::T # duality measure
end
\end{dispjulia}

Along with the solver and iterates, we have the following enums that track the
their statuses. If a \texttt{verbose} option is added, these will be more useful
in determining the reason why a linear program could not be solved.
\begin{dispjulia}
@enum SolverStatus begin
    SolverUndefined

    # problem status
    SolverOptimal
    SolverPrimalInfeasible
    SolverDualInfeasible

    # computation status
    SolverExceededIterations
    SolverNumericalInstability
    SolverExceededMemory
end

@enum IterateStatus begin
    IterateUndefined

    IterateOptimal
    IterateFeasible
    IterateInfeasible
end
\end{dispjulia}

\texttt{Residuals} includes all the residual values required to check for
optimality or infeasibility. These are recomputed and update appropriately at
the start of each iteration.
\begin{dispjulia}
mutable struct Residual{T <: Real}
  rp::T # $\tau$ * b - A * x
  ru::T # $\tau$ * u - v - U * x
  rd::T # $\tau$ * c - A' * y - s + U * x
  rg::T # c' * x - b' * $\lambda$ - u * w + $\kappa$

  # norms of above residuals
  rpn::T
  run::T
  rdn::T
  rgn::T
end
\end{dispjulia}

The final struct \texttt{Tolerances} logs the tolerances for each of the
positive parameters.
\begin{dispjulia}
mutable struct Tolerances{T}
    $\varepsilon$p::T
    $\varepsilon$d::T
    $\varepsilon$g::T
    $\varepsilon$i::T
end
\end{dispjulia}

\subsection{Performance Notes}\label{sec:perf-notes}
This section is for general notes on writing performant code in Julia.

First, it is important to write "type-stable" functions and \texttt{struct}s.
As such, when creating generic (or parameterized) types, it is best to
avoid fields with abstract types; i.e. annotate types in \texttt{struct}s, for
example, so that the compiler can generate high performance code.

Second, it is best to pre-allocate memory as much as possible because
dynamically allocating memory can lead to expensive operations, such a resizing
an array. Thus, most of the functions in PDIPS.jl modify their arguments instead
of returning a new object. Loosely related is the idea of using \texttt{@views}
instead of slicing arrays.

Third, use the \texttt{dot} operator where appropriate when applying basic
operations to vectors. For example, for \texttt{Vector}s \texttt{x} and
\texttt{y}, use
\begin{dispjulia}
x .+ y
\end{dispjulia}
as opposed to
\begin{dispjulia}
x + y
\end{dispjulia}
even though the latter is valid.

Finally, make use of specialized linear algebra routines (BLAS) when
applicable since they are optimized for common linear algebra operations.
For instance, take the operation \(\mY = a\mX + \mY\). It is valid
to do either of the following.
\begin{dispjulia}
Y = a * X + Y
\end{dispjulia}
\begin{dispjulia}
Y .+= a .* X
\end{dispjulia}
However, a higher-performance option is
\begin{dispjulia}
axpy!(a, X, Y)
\end{dispjulia}
because it avoids miscellaneous allocations (among other things).

\section{Benchmarks}\label{sec:benchmarks}
\newthought{In this section,} we see the computational results of the package
in terms of memory used, allocations made, and time taken. The code was run
using Julia v1.4 on a Intel i7 5820K, overclocked at 4.5GHz, 32GB RAM machine
running Ubuntu. The information reported in Table \ref{tab:results} does not
include preprocessing (only the main iterations).
\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \begin{tabular}{l c c c c}
        \toprule
        Name & Optimal & Memory & Allocations & Mean Time  \\
        \midrule
        25fv47 & Y & 48.97 MiB & 4510 & 88.956 ms \\
        adlittle & Y & 1.47 MiB & 2014 & 935.618 \(\mu\)s \\
        afiro & Y & 433.97 KiB & 1456 & 272.505 \(\mu\)s \\
        agg & Y & 11.75 MiB & 3054 & 8.408 ms \\
        brandy & Y & 6.50 MiB & 2884 & 5.904 ms \\
        chemcom & N & 4.58 MiB & 1239 & 3.563 ms \\
        fit1d & Y & 27.16 MiB & 3050 & 18.282 ms \\
        ganges & Y & 33.17 MiB & 3336 & 26.077 ms \\
        stocfor1 & Y & 2.35 MiB & 2436 & 1.532 ms \\
        \bottomrule
    \end{tabular}
    \caption{Computational results of PDIPS.jl}
    \label{tab:results}
\end{table}

\newpage
\bibliographystyle{plainnat}
\bibliography{PDIPS}

\end{document}
